{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNzdSQVo9l_q"
   },
   "source": [
    "# Рекуррентные нейросети\n",
    "\n",
    "Построим простейшую нейросеть для посимвольной генерации текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-12T15:11:51.143288Z",
     "start_time": "2020-03-12T15:11:49.649354Z"
    },
    "id": "uB1QEk1gm1oW"
   },
   "outputs": [],
   "source": [
    "import pandas as pd  # для работы с данными\n",
    "import time  # для оценки времени\n",
    "import torch  # для написания нейросети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3aOx64Mm1oZ"
   },
   "source": [
    "## Загрузка данных\n",
    "\n",
    "Будем работать с датасетом реплик из Симпсонов. Нам нужно извлечь предобработанные тексты и закодировать их числами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-12T15:12:00.685533Z",
     "start_time": "2020-03-12T15:12:00.591616Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 613
    },
    "id": "duHW4TnGm1ob",
    "outputId": "2c6640cd-d0f3-4857-a3e2-79e7d82c917c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ds/69y_yy712js2_g3r477nw9s40000gn/T/ipykernel_12475/187986918.py:1: DtypeWarning: Columns (4,5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('simpsons_script_lines.csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>episode_id</th>\n",
       "      <th>number</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>timestamp_in_ms</th>\n",
       "      <th>speaking_line</th>\n",
       "      <th>character_id</th>\n",
       "      <th>location_id</th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>raw_location_text</th>\n",
       "      <th>spoken_words</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9549</td>\n",
       "      <td>32</td>\n",
       "      <td>209</td>\n",
       "      <td>Miss Hoover: No, actually, it was a little of ...</td>\n",
       "      <td>848000</td>\n",
       "      <td>True</td>\n",
       "      <td>464.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>Springfield Elementary School</td>\n",
       "      <td>No, actually, it was a little of both. Sometim...</td>\n",
       "      <td>no actually it was a little of both sometimes ...</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9550</td>\n",
       "      <td>32</td>\n",
       "      <td>210</td>\n",
       "      <td>Lisa Simpson: (NEAR TEARS) Where's Mr. Bergstrom?</td>\n",
       "      <td>856000</td>\n",
       "      <td>True</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Springfield Elementary School</td>\n",
       "      <td>Where's Mr. Bergstrom?</td>\n",
       "      <td>wheres mr bergstrom</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9551</td>\n",
       "      <td>32</td>\n",
       "      <td>211</td>\n",
       "      <td>Miss Hoover: I don't know. Although I'd sure l...</td>\n",
       "      <td>856000</td>\n",
       "      <td>True</td>\n",
       "      <td>464.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>Springfield Elementary School</td>\n",
       "      <td>I don't know. Although I'd sure like to talk t...</td>\n",
       "      <td>i dont know although id sure like to talk to h...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9552</td>\n",
       "      <td>32</td>\n",
       "      <td>212</td>\n",
       "      <td>Lisa Simpson: That life is worth living.</td>\n",
       "      <td>864000</td>\n",
       "      <td>True</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Springfield Elementary School</td>\n",
       "      <td>That life is worth living.</td>\n",
       "      <td>that life is worth living</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9553</td>\n",
       "      <td>32</td>\n",
       "      <td>213</td>\n",
       "      <td>Edna Krabappel-Flanders: The polls will be ope...</td>\n",
       "      <td>864000</td>\n",
       "      <td>True</td>\n",
       "      <td>40.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Edna Krabappel-Flanders</td>\n",
       "      <td>Springfield Elementary School</td>\n",
       "      <td>The polls will be open from now until the end ...</td>\n",
       "      <td>the polls will be open from now until the end ...</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  episode_id  number  \\\n",
       "0  9549          32     209   \n",
       "1  9550          32     210   \n",
       "2  9551          32     211   \n",
       "3  9552          32     212   \n",
       "4  9553          32     213   \n",
       "\n",
       "                                            raw_text timestamp_in_ms  \\\n",
       "0  Miss Hoover: No, actually, it was a little of ...          848000   \n",
       "1  Lisa Simpson: (NEAR TEARS) Where's Mr. Bergstrom?          856000   \n",
       "2  Miss Hoover: I don't know. Although I'd sure l...          856000   \n",
       "3           Lisa Simpson: That life is worth living.          864000   \n",
       "4  Edna Krabappel-Flanders: The polls will be ope...          864000   \n",
       "\n",
       "  speaking_line character_id  location_id       raw_character_text  \\\n",
       "0          True        464.0          3.0              Miss Hoover   \n",
       "1          True          9.0          3.0             Lisa Simpson   \n",
       "2          True        464.0          3.0              Miss Hoover   \n",
       "3          True          9.0          3.0             Lisa Simpson   \n",
       "4          True         40.0          3.0  Edna Krabappel-Flanders   \n",
       "\n",
       "               raw_location_text  \\\n",
       "0  Springfield Elementary School   \n",
       "1  Springfield Elementary School   \n",
       "2  Springfield Elementary School   \n",
       "3  Springfield Elementary School   \n",
       "4  Springfield Elementary School   \n",
       "\n",
       "                                        spoken_words  \\\n",
       "0  No, actually, it was a little of both. Sometim...   \n",
       "1                             Where's Mr. Bergstrom?   \n",
       "2  I don't know. Although I'd sure like to talk t...   \n",
       "3                         That life is worth living.   \n",
       "4  The polls will be open from now until the end ...   \n",
       "\n",
       "                                     normalized_text word_count  \n",
       "0  no actually it was a little of both sometimes ...         31  \n",
       "1                                wheres mr bergstrom          3  \n",
       "2  i dont know although id sure like to talk to h...         22  \n",
       "3                          that life is worth living          5  \n",
       "4  the polls will be open from now until the end ...         33  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('simpsons_script_lines.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-12T15:12:15.820742Z",
     "start_time": "2020-03-12T15:12:15.809523Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gjSdDLcSm1oc",
    "outputId": "6c4b9cb4-832b-427a-a6ba-7deca7c9c216"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['no actually it was a little of both sometimes when a disease is in all the magazines and all the news shows its only natural that you think you have it',\n",
       " 'wheres mr bergstrom',\n",
       " 'i dont know although id sure like to talk to him he didnt touch my lesson plan what did he teach you',\n",
       " 'that life is worth living',\n",
       " 'the polls will be open from now until the end of recess now just in case any of you have decided to put any thought into this well have our final statements martin',\n",
       " 'i dont think theres anything left to say',\n",
       " 'bart',\n",
       " 'victory party under the slide',\n",
       " nan,\n",
       " 'mr bergstrom mr bergstrom']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases = df['normalized_text'].tolist()  # колонка с предобработанными текстами\n",
    "phrases[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-12T15:12:32.809563Z",
     "start_time": "2020-03-12T15:12:32.768140Z"
    },
    "id": "1ChXCEWwm1od"
   },
   "outputs": [],
   "source": [
    "text = [[c for c in ph] for ph in phrases if type(ph) is str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAQEsntvm1od"
   },
   "source": [
    "## Создаём массив с данными\n",
    "\n",
    "Нужно\n",
    "\n",
    "1. Разбить данные на токены (у нас символы)\n",
    "2. Закодировать числами\n",
    "3. Превратить в эмбеддинги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-12T15:12:53.641351Z",
     "start_time": "2020-03-12T15:12:53.636757Z"
    },
    "id": "ZnCuSmHym1od"
   },
   "outputs": [],
   "source": [
    "CHARS = set('abcdefghijklmnopqrstuvwxyz ')  # все символы, которые мы хотим использовать для кодировки = наш словарь\n",
    "INDEX_TO_CHAR = ['none'] + [w for w in CHARS]  # все неизвестные символы будут получать тег none\n",
    "CHAR_TO_INDEX = {w: i for i, w in enumerate(INDEX_TO_CHAR)}  # словарь токен-индекс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'none': 0,\n",
       " ' ': 1,\n",
       " 'w': 2,\n",
       " 'm': 3,\n",
       " 'd': 4,\n",
       " 'y': 5,\n",
       " 'u': 6,\n",
       " 'a': 7,\n",
       " 'b': 8,\n",
       " 'p': 9,\n",
       " 'c': 10,\n",
       " 'k': 11,\n",
       " 'n': 12,\n",
       " 'h': 13,\n",
       " 'v': 14,\n",
       " 'e': 15,\n",
       " 't': 16,\n",
       " 'q': 17,\n",
       " 'z': 18,\n",
       " 'l': 19,\n",
       " 'g': 20,\n",
       " 'x': 21,\n",
       " 'j': 22,\n",
       " 'f': 23,\n",
       " 's': 24,\n",
       " 'i': 25,\n",
       " 'r': 26,\n",
       " 'o': 27}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHAR_TO_INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vp5hyfQMIXUe",
    "outputId": "e06f0b3d-cef5-4fc8-8105-7139f9e264e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(INDEX_TO_CHAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-12T15:13:58.918002Z",
     "start_time": "2020-03-12T15:13:55.602551Z"
    },
    "id": "Z8_Kqpjnm1oe"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 50  # мы хотим ограничить максимальную длину ввода\n",
    "X = torch.zeros((len(text), MAX_LEN), dtype=int)  # создаём пустой вектор для текста, чтобы класть в него индексы токенов\n",
    "for i in range(len(text)):  # для каждого предложения\n",
    "    for j, w in enumerate(text[i]):  # для каждого токена\n",
    "        if j >= MAX_LEN:\n",
    "            break\n",
    "        X[i, j] = CHAR_TO_INDEX.get(w, CHAR_TO_INDEX['none'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-12T15:14:24.776980Z",
     "start_time": "2020-03-12T15:14:24.761649Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UZ_pRDrHm1of",
    "outputId": "9734429d-3a11-4f24-91d0-c8e7e1b6e64a",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12, 27,  1,  7, 10, 16,  6,  7, 19, 19,  5,  1, 25, 16,  1,  2,  7, 24,\n",
       "          1,  7,  1, 19, 25, 16, 16, 19, 15,  1, 27, 23,  1,  8, 27, 16, 13,  1,\n",
       "         24, 27,  3, 15, 16, 25,  3, 15, 24,  1,  2, 13, 15, 12],\n",
       "        [ 2, 13, 15, 26, 15, 24,  1,  3, 26,  1,  8, 15, 26, 20, 24, 16, 26, 27,\n",
       "          3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [25,  1,  4, 27, 12, 16,  1, 11, 12, 27,  2,  1,  7, 19, 16, 13, 27,  6,\n",
       "         20, 13,  1, 25,  4,  1, 24,  6, 26, 15,  1, 19, 25, 11, 15,  1, 16, 27,\n",
       "          1, 16,  7, 19, 11,  1, 16, 27,  1, 13, 25,  3,  1, 13],\n",
       "        [16, 13,  7, 16,  1, 19, 25, 23, 15,  1, 25, 24,  1,  2, 27, 26, 16, 13,\n",
       "          1, 19, 25, 14, 25, 12, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [16, 13, 15,  1,  9, 27, 19, 19, 24,  1,  2, 25, 19, 19,  1,  8, 15,  1,\n",
       "         27,  9, 15, 12,  1, 23, 26, 27,  3,  1, 12, 27,  2,  1,  6, 12, 16, 25,\n",
       "         19,  1, 16, 13, 15,  1, 15, 12,  4,  1, 27, 23,  1, 26]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_j4Fpnr2m1of"
   },
   "source": [
    "## Embedding и RNN ячейки\n",
    "\n",
    "Каждому токену мы хотим сопоставить не просто число, но вектор. Поэтому вектор текста нам нужно умножить на матрицу эмбеддингов, которая тоже будет учиться в процессе обучения нейросети. Для создания такой матрицы нам нужен слой `nn.Embedding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "49DO7kkNqtVK",
    "outputId": "45118f6e-d023-459c-ce30-c86567e1e06f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 50])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-12T15:14:15.694973Z",
     "start_time": "2020-03-12T15:14:15.644024Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "grMla_CJm1og",
    "outputId": "5e59a467-0e56-4686-823e-0c34f490c606"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 50, 28])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = torch.nn.Embedding(len(INDEX_TO_CHAR), 28)  # размер словаря * размер вектора для кодировки каждого слова\n",
    "t = embeddings(X[0:5])\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3689,  0.3359,  0.1294,  ..., -2.2670, -0.5084,  2.0914],\n",
       "         [-0.0124, -0.4274,  0.0569,  ..., -0.8373,  0.2788,  0.2409],\n",
       "         [ 0.4870, -0.6032,  0.2132,  ...,  1.2992, -1.1362, -0.7522],\n",
       "         ...,\n",
       "         [ 1.1295,  0.6400,  0.3355,  ...,  0.5713,  0.1381, -1.9632],\n",
       "         [-1.7195, -1.1320, -0.4221,  ..., -1.0422, -2.1570, -0.1842],\n",
       "         [ 0.3689,  0.3359,  0.1294,  ..., -2.2670, -0.5084,  2.0914]],\n",
       "\n",
       "        [[ 0.6711, -0.1875,  0.3068,  ..., -1.3658, -0.1113,  0.1579],\n",
       "         [ 1.1295,  0.6400,  0.3355,  ...,  0.5713,  0.1381, -1.9632],\n",
       "         [-1.7195, -1.1320, -0.4221,  ..., -1.0422, -2.1570, -0.1842],\n",
       "         ...,\n",
       "         [ 2.1302,  0.0217, -1.4572,  ...,  0.0211,  0.0140, -0.3286],\n",
       "         [ 2.1302,  0.0217, -1.4572,  ...,  0.0211,  0.0140, -0.3286],\n",
       "         [ 2.1302,  0.0217, -1.4572,  ...,  0.0211,  0.0140, -0.3286]],\n",
       "\n",
       "        [[ 0.5599, -0.4547, -0.1584,  ..., -1.0144,  0.6576, -0.3028],\n",
       "         [ 0.4870, -0.6032,  0.2132,  ...,  1.2992, -1.1362, -0.7522],\n",
       "         [ 2.1465,  1.3735, -0.5997,  ...,  1.0677, -0.1743, -0.3870],\n",
       "         ...,\n",
       "         [ 1.6896,  0.5826, -1.6059,  ...,  0.5174,  0.9550,  0.5873],\n",
       "         [ 0.4870, -0.6032,  0.2132,  ...,  1.2992, -1.1362, -0.7522],\n",
       "         [ 1.1295,  0.6400,  0.3355,  ...,  0.5713,  0.1381, -1.9632]],\n",
       "\n",
       "        [[-0.8801, -1.2740,  1.1610,  ...,  1.1861, -0.4451, -0.5345],\n",
       "         [ 1.1295,  0.6400,  0.3355,  ...,  0.5713,  0.1381, -1.9632],\n",
       "         [-0.5824, -1.1401, -0.0095,  ..., -0.1410,  1.0680, -0.6664],\n",
       "         ...,\n",
       "         [ 2.1302,  0.0217, -1.4572,  ...,  0.0211,  0.0140, -0.3286],\n",
       "         [ 2.1302,  0.0217, -1.4572,  ...,  0.0211,  0.0140, -0.3286],\n",
       "         [ 2.1302,  0.0217, -1.4572,  ...,  0.0211,  0.0140, -0.3286]],\n",
       "\n",
       "        [[-0.8801, -1.2740,  1.1610,  ...,  1.1861, -0.4451, -0.5345],\n",
       "         [ 1.1295,  0.6400,  0.3355,  ...,  0.5713,  0.1381, -1.9632],\n",
       "         [-1.7195, -1.1320, -0.4221,  ..., -1.0422, -2.1570, -0.1842],\n",
       "         ...,\n",
       "         [-0.4931, -0.6508, -0.0044,  ...,  1.8126, -0.1629,  0.5549],\n",
       "         [ 0.4870, -0.6032,  0.2132,  ...,  1.2992, -1.1362, -0.7522],\n",
       "         [-0.6765, -1.5170,  0.4475,  ..., -0.1689, -1.8613, -0.2345]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-12T15:14:48.405046Z",
     "start_time": "2020-03-12T15:14:48.400041Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "28tafLrcm1og",
    "outputId": "800c5658-b33e-49f6-9854-d5b9862fe141"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 50, 28]), torch.Size([5, 50]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape, X[0:5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-12T15:18:23.190978Z",
     "start_time": "2020-03-12T15:18:23.180493Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PdGchJjim1oh",
    "outputId": "c1b1e6ca-d6b1-4a65-89cd-b334754537c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 50, 128]), torch.Size([1, 5, 128]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = torch.nn.RNN(28, 128, batch_first=True)  # на вход - размер эмбеддинга, размер скрытого состояния и порядок размерностей\n",
    "o, s = rnn(t)\n",
    "# вектора для слов: батч * число токенов * размер скрытого состояния\n",
    "# вектор скрытого состояния: число вектров (один) * батч * размер скрытого состояния\n",
    "o.shape, s.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n7JUp4QuCQZu"
   },
   "source": [
    "Можно применять несколько рекуррентных ячеек подряд"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-12T15:18:35.190131Z",
     "start_time": "2020-03-12T15:18:35.180708Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kdLW-BSsm1oh",
    "outputId": "c385efe3-764c-4bcd-93b8-ef4d8a68f85c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 50, 128]), torch.Size([1, 5, 128]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o, s2 = rnn(t, s)\n",
    "o.shape, s2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghow-s0Gm1oi"
   },
   "source": [
    "## Реализация сети с RNN\n",
    "3 слоя:\n",
    "1. Embeding (30)\n",
    "2. RNN (hidden_dim=128)\n",
    "3. Полносвязный слой для предсказания буквы (28, то есть размер словаря)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-12T15:26:45.716418Z",
     "start_time": "2020-03-12T15:26:45.710937Z"
    },
    "id": "Z9toGCkOm1oi"
   },
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(28, 30)\n",
    "        self.rnn = torch.nn.RNN(30, 128)\n",
    "        self.out = torch.nn.Linear(128, 28)\n",
    "\n",
    "    def forward(self, sentences, state=None):\n",
    "        x = self.embedding(sentences)\n",
    "        x, s = self.rnn(x) # берём выход с последнего слоя для всех токенов, а не скрытое состояние\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-12T15:20:53.260599Z",
     "start_time": "2020-03-12T15:20:53.256979Z"
    },
    "id": "f6aCazQOm1oi"
   },
   "outputs": [],
   "source": [
    "model = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-12T15:21:01.929404Z",
     "start_time": "2020-03-12T15:21:01.925999Z"
    },
    "id": "HmdNPJEWm1oj"
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()  # типичный лосс многоклассовой классификации\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-J6XXp0gDUOO"
   },
   "source": [
    "Обучение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-12T15:22:04.410583Z",
     "start_time": "2020-03-12T15:21:34.734119Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o7zbPtfLm1oj",
    "outputId": "0a5e6dee-4356-4350-8c23-ab8ca1542eaa",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Time: 16.475, Train loss: 1.826\n",
      "Epoch 1. Time: 16.602, Train loss: 1.727\n",
      "Epoch 2. Time: 16.502, Train loss: 1.712\n",
      "Epoch 3. Time: 16.549, Train loss: 1.703\n",
      "Epoch 4. Time: 16.506, Train loss: 1.695\n",
      "Epoch 5. Time: 16.520, Train loss: 1.688\n",
      "Epoch 6. Time: 16.545, Train loss: 1.684\n",
      "Epoch 7. Time: 16.511, Train loss: 1.680\n",
      "Epoch 8. Time: 16.507, Train loss: 1.677\n",
      "Epoch 9. Time: 16.500, Train loss: 1.674\n",
      "Epoch 10. Time: 16.581, Train loss: 1.672\n",
      "Epoch 11. Time: 16.560, Train loss: 1.670\n",
      "Epoch 12. Time: 16.750, Train loss: 1.669\n",
      "Epoch 13. Time: 16.698, Train loss: 1.667\n",
      "Epoch 14. Time: 16.963, Train loss: 1.666\n",
      "Epoch 15. Time: 16.794, Train loss: 1.665\n",
      "Epoch 16. Time: 16.501, Train loss: 1.664\n",
      "Epoch 17. Time: 16.474, Train loss: 1.663\n",
      "Epoch 18. Time: 16.841, Train loss: 1.662\n",
      "Epoch 19. Time: 16.694, Train loss: 1.664\n"
     ]
    }
   ],
   "source": [
    "for ep in range(20):\n",
    "    start = time.time()\n",
    "    train_loss = 0.\n",
    "    train_passed = 0\n",
    "\n",
    "    for i in range(int(len(X) / 100)):\n",
    "        # берём батч в 100 элементов\n",
    "        batch = X[i * 100:(i + 1) * 100]\n",
    "        X_batch = batch[:, :-1]\n",
    "        Y_batch = batch[:, 1:].flatten()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        answers = model.forward(X_batch)\n",
    "        answers = answers.view(-1, len(INDEX_TO_CHAR))\n",
    "        loss = criterion(answers, Y_batch)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_passed += 1\n",
    "\n",
    "    print(\"Epoch {}. Time: {:.3f}, Train loss: {:.3f}\".format(ep, time.time() - start, train_loss / train_passed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-12T15:28:40.363097Z",
     "start_time": "2020-03-12T15:28:40.357998Z"
    },
    "id": "xP_GQWpkm1ok"
   },
   "source": [
    "## Генерация\n",
    "\n",
    "\n",
    "- Сначала отправлем в модель буквы из предложения (прогревая состояние)\n",
    "- Затем берём самую вероятную букву и добавляем её в предложение\n",
    "- Повторяем пока не получим none (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ybcHzCbUJs-R",
    "outputId": "0bbd0899-1325-4627-9bac-9dcc66a9ad41"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHAR_TO_INDEX['none']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "PuqijUpeL3eB"
   },
   "outputs": [],
   "source": [
    "def generate_sentence(word):\n",
    "    sentence = list(word)\n",
    "    sentence = [CHAR_TO_INDEX.get(s, 0) for s in sentence]\n",
    "    answers = model.forward(torch.tensor(sentence))\n",
    "    probas, indices = answers.topk(1)\n",
    "    return ''.join([INDEX_TO_CHAR[ind.item()] for ind in indices.flatten()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 58
    },
    "id": "YpjZvuR_L92Z",
    "outputId": "84713f2b-6eef-451e-a28c-ea670e1e193f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' uo'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence('dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 58
    },
    "id": "xqS7Y-z1HghE",
    "outputId": "2104f5a2-2f36-41e4-8ee5-4aeb2a63e1d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' uita  tll t  t t tttls    iu teu e'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence('no actually it was a little of both')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание про шифр Цезаря"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция шифрования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caesar(sym: str, k: int) -> str:\n",
    "    current_ord = ord(sym)\n",
    "    # print(current_ord)\n",
    "    if current_ord > 120:\n",
    "        current_ord -= 26\n",
    "    return chr(current_ord + k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caesar('g', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting english_words\n",
      "  Downloading english_words-2.0.2-py3-none-any.whl.metadata (3.6 kB)\n",
      "Downloading english_words-2.0.2-py3-none-any.whl (8.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: english_words\n",
      "Successfully installed english_words-2.0.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install english_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from english_words import get_english_words_set\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(get_english_words_set(['web2'], alpha=True, lower=True))\n",
    "random_10k_words = random.sample(words, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = random_10k_words\n",
    "encrypted_words = []\n",
    "for word in words:\n",
    "    enc_word = ''\n",
    "    for sym in word:\n",
    "        enc_word += caesar(sym, 2)\n",
    "    encrypted_words.append(enc_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['volplanist', 'psoadic', 'contraindicative', 'scatteration', 'forehill']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xqnrncpkuv', 'ruqcfke', 'eqpvtckpfkecvkxg', 'uecvvgtcvkqp', 'hqtgjknn']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encrypted_words[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHARS = set('abcdefghijklmnopqrstuvwxyz ')  # все символы, которые мы хотим использовать для кодировки = наш словарь\n",
    "INDEX_TO_CHAR = ['none'] + [w for w in CHARS]  # все неизвестные символы будут получать тег none\n",
    "CHAR_TO_INDEX = {w: i for i, w in enumerate(INDEX_TO_CHAR)}  # словарь токен-индекс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 20\n",
    "X = torch.zeros((len(words), MAX_LEN), dtype=int)  # создаём пустой вектор для текста, чтобы класть в него индексы токенов\n",
    "for i in range(len(words)):  # для каждого слова\n",
    "    for j, w in enumerate(words[i]):  # для каждого токена\n",
    "        if j >= MAX_LEN:\n",
    "            break\n",
    "        X[i, j] = CHAR_TO_INDEX.get(w, CHAR_TO_INDEX['none'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = torch.zeros((len(encrypted_words), MAX_LEN), dtype=int)  # создаём пустой вектор для текста, чтобы класть в него индексы токенов\n",
    "for i in range(len(encrypted_words)):  # для каждого слова\n",
    "    for j, w in enumerate(encrypted_words[i]):  # для каждого токена\n",
    "        if j >= MAX_LEN:\n",
    "            break\n",
    "        Y[i, j] = CHAR_TO_INDEX.get(w, CHAR_TO_INDEX['none'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14, 27, 19,  9, 19,  7, 12, 25, 24, 16,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0],\n",
       "        [ 9, 24, 27,  7,  4, 25, 10,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0],\n",
       "        [10, 27, 12, 16, 26,  7, 25, 12,  4, 25, 10,  7, 16, 25, 14, 15,  0,  0,\n",
       "          0,  0],\n",
       "        [24, 10,  7, 16, 16, 15, 26,  7, 16, 25, 27, 12,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0],\n",
       "        [23, 27, 26, 15, 13, 25, 19, 19,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[21, 17, 12, 26, 12, 10,  9, 11,  6, 14,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0],\n",
       "        [26,  6, 17, 10, 23, 11, 15,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0],\n",
       "        [15, 17,  9, 14, 16, 10, 11,  9, 23, 11, 15, 10, 14, 11, 21, 20,  0,  0,\n",
       "          0,  0],\n",
       "        [ 6, 15, 10, 14, 14, 20, 16, 10, 14, 11, 17,  9,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0],\n",
       "        [13, 17, 16, 20, 22, 11, 12, 12,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_count = 8000 # и еще 2000 на тест\n",
    "X_train = X[:train_count]\n",
    "X_test = X[train_count:]\n",
    "Y_train = Y[:train_count]\n",
    "Y_test = Y[train_count:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(28, 30)\n",
    "        self.rnn = torch.nn.RNN(30, 128)\n",
    "        self.out = torch.nn.Linear(128, 28)\n",
    "\n",
    "    def forward(self, sentences, state=None):\n",
    "        x = self.embedding(sentences)\n",
    "        x, s = self.rnn(x) # берём выход с последнего слоя для всех токенов, а не скрытое состояние\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()  # типичный лосс многоклассовой классификации\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Time: 0.806, Train loss: 1.080\n",
      "Epoch 1. Time: 0.784, Train loss: -0.059\n",
      "Epoch 2. Time: 0.786, Train loss: 0.234\n",
      "Epoch 3. Time: 0.788, Train loss: 0.369\n",
      "Epoch 4. Time: 0.786, Train loss: 0.852\n",
      "Epoch 5. Time: 0.789, Train loss: 0.272\n",
      "Epoch 6. Time: 0.788, Train loss: 0.345\n",
      "Epoch 7. Time: 0.796, Train loss: 0.608\n",
      "Epoch 8. Time: 0.793, Train loss: -0.455\n",
      "Epoch 9. Time: 0.788, Train loss: 0.383\n",
      "Epoch 10. Time: 0.792, Train loss: 0.017\n",
      "Epoch 11. Time: 0.806, Train loss: -0.496\n",
      "Epoch 12. Time: 0.800, Train loss: 0.614\n",
      "Epoch 13. Time: 0.795, Train loss: 0.352\n",
      "Epoch 14. Time: 0.795, Train loss: 0.492\n",
      "Epoch 15. Time: 0.803, Train loss: -0.155\n",
      "Epoch 16. Time: 0.804, Train loss: -1.978\n",
      "Epoch 17. Time: 0.803, Train loss: -5.320\n",
      "Epoch 18. Time: 0.795, Train loss: 1.576\n",
      "Epoch 19. Time: 0.795, Train loss: 5.787\n"
     ]
    }
   ],
   "source": [
    "for ep in range(20):\n",
    "    start = time.time()\n",
    "    train_loss = 0.\n",
    "    train_passed = 0\n",
    "\n",
    "    for i in range(int(len(X_train) / 100)):\n",
    "        # берём батч в 100 элементов\n",
    "        X_batch = X_train[i * 100:(i + 1) * 100]\n",
    "        Y_batch = Y_train[i * 100:(i + 1) * 100]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        answers = model.forward(X_batch)\n",
    "        embeddings = torch.nn.Embedding(len(INDEX_TO_CHAR), 28)\n",
    "        loss = criterion(answers, embeddings(Y_batch))\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_passed += 1\n",
    "\n",
    "    print(\"Epoch {}. Time: {:.3f}, Train loss: {:.3f}\".format(ep, time.time() - start, train_loss / train_passed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
